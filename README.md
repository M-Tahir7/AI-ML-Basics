________________________________________

AI (Artificial Intelligence) is a domain of computer science in which we mimic the thinking and understanding process of the human brain.
Its goal is to create a digital brain that can think like humans, make decisions, and solve problems.

ğŸ“Œ Example:
If you ask Siri or Google Assistant something and it responds, then it is using AI.

________________________________________

 In simple words, it tells us what is the best thing to do in any situation to get the best result.

Imagine a student is preparing for an exam. He tries different strategies:

Sometimes he studies late at night.

Sometimes he studies early in the morning.

Sometimes he uses YouTube videos.

Sometimes he reads textbooks.

Over time, he notices this:

ğŸ“Œ â€œWhenever I wake up early and revise with YouTube videos, I score higher in quizzes.â€

Asking kids open-ended questions like â€œWhatâ€™s your favorite color?â€ gives different answers â€” like normal AI, which gives varied responses.

But if you give them a math question like â€œ(3 + 2) Ã— 4â€, they all say â€œ20â€ â€” like Q*, which finds the single best answer in any situation.

So he learns:
âœ… Best Action = Wake up early + Watch videos

________________________________________

Machine Learning (ML) says:

"Just give me inputs and outputs (past data), and Iâ€™ll learn the pattern behind them. Once I learn the pattern, I can predict answers for future inputs."

Deep Learning (DL) goes a step further:

"Iâ€™m inspired by how the human brain works. I use artificial 'neurons' (which are actually mathematical functions), and when many of them are connected together in layers, we call that a neural network."

________________________________________

Labelled data is the type of data where each input is provided along with its correct output (label).

ğŸ“Œ Example:
If you're shown an image of an apple and told that "this is an apple," then that is labelled data.
The machine learns from such labelled data, like:
Image â†’ Apple
Image â†’ Not Apple

________________________________________

Unlabelled data is the type of data where inputs are provided without any labels â€” meaning, we don't know the correct output for each input.

ğŸ“Œ Example:
Images stored in your phone gallery are unlabelled because they are not tagged with what they contain (like "apple", "car", or "person").
Most of the data in the real world is unlabelled.

________________________________________

Structured data is the type of data that is organized in rows and columns, making it easy to store and analyze â€” just like in an Excel spreadsheet.
ğŸ“Œ Example:
An Excel file where each row represents a person and each column represents features like Name, Age, and Gender.


Unstructured data is data that does not have a organized structure like rows and columns. It includes things like texts, images, videos, and speech.

Example:

Emails or written documents

Photos stored on your phone

Audio clips

Video recordings

________________________________________

Reinforcement Learning Explanation (Based on Example)
Scenario: Gharwale bike chalana sikhate hain. Rules batate hain. Environment define karte hain. Kab brake lagani hai, kya sahi hai kya nahi.

ğŸ§  Mapping to RL Concepts:
Real-life Term	RL Term
Gharwale rules batate hain	Environment + Reward policy
"Ghar ke andar nahi chalani"	Environment constraint
Brake, race, kick sikhana	Action space
Aram se drive karna	Optimal policy learned
Wheeling karna	Undesirable behavior
Gharwale mana karte hain	Negative reward (punishment)
Baccha wheeling chhor deta hai	Unlearning due to penalty

________________________________________

ğŸ” Learning + Unlearning:
Jab baccha aram aram se chalata hai â†’ positive reward milta hai (tareef, encouragement).

Jab wo wheeling karta hai â†’ negative reward (daant, punishment).

Jab ye negative feedback repeat hota hai â†’ wo unlearn karta hai wheeling ko.

Reinforcement Learning mein agent action karta hai, environment usay reward ya punishment deta hai, aur agent apna behavior update karta hai.

ğŸ“Œ Summary:
Learning = Bike chalana sikhna through positive reinforcement.

Unlearning = Wheeling chhodna due to repeated negative reinforcement.

RL Concept = Agent interacts with environment, learns from reward and punishment.

_______________________________________

âœ… Supervised Learning
Thereâ€™s a small child at my home, and I often teach him the names of different fruits:

ğŸ "This is an apple"
ğŸŒ "This is a banana"
ğŸŠ "This is an orange"

One day, I showed him an apple and asked:
"What is this?"
He replied confidently:
"It's an apple!"

_______________________________________

Unsupervised Learning
Now imagine I send someone to the market with this instruction:
"Buy fruits or vegetables you like."

ğŸ›’ They go to the store and pick whatever seems good to them, based on their personal preferences â€“ without being told what to buy specifically.

ğŸ“Œ This represents Unsupervised Learning â€“ where the model doesnâ€™t get labeled data but finds patterns or groups things on its own.

________________________________________

Deep Learning Overview
In deep learning, we can still apply supervised, unsupervised, or reinforcement learning, but the difference is that the model is trained using artificial neurons.

What is a Neuron?
A neuron in deep learning is a mathematical function.
For example, one simple function is the Relu neuron (max function):
â€¢	It takes two numbers as input.
â€¢	It outputs the larger number.
This is similar to how biological neurons in the human brain take inputs, process them, and pass outputs to other neurons.
________________________________________
Connecting Neurons
Just like the human brain has billions of interconnected neurons that make decisions, in deep learning we connect artificial neurons to form a neural network.
This arrangement of neurons can have different architectures depending on the problem.
________________________________________
Data in ML vs. DL
â€¢	Machine Learning (ML) works well with structured data (tables, rows, columns).
â€¢	Deep Learning (DL) can work with both structured and unstructured data(Text , Image , Voice):
o	Text â†’ becomes Natural Language Processing (NLP)
o	Images â†’ becomes Computer Vision
o	Voice/Speech â†’ becomes Speech Recognition

________________________________________

Discriminative AI vs Generative AI
1. Discriminative AI
â€¢	Definition:
Focuses on distinguishing between different classes or categories.
It learns patterns in data to decide â€œWhat class does this belong to?â€
â€¢	Goal: Classification or prediction.
â€¢	Example:
o	Email spam detection (Spam or Not Spam)
o	Image classification (Cat or Dog)
________________________________________
2. Generative AI
â€¢	Definition:
Focuses on creating new data that resembles the original data.
It may use discriminative AI techniques internally to understand patterns, but its main goal is generation, not classification.
â€¢	Example:
o	ChatGPT writing an essay
o	 Creating Ghibli image from text
o	AI making new music

________________________________________

In deep learning, the word deep comes from the depth of the neural network, meaning:
â€¢	We donâ€™t just have one or two layers of neurons (like in simple neural networks)
â€¢	Instead, we have many hidden layers between the input and output layers.
â€¢	More layers = more neurons = the model can learn more complex patterns.
So, â€œdeepâ€ is about layer depth
For example:
â€¢	Shallow Neural Network â†’ 1â€“2 hidden layers
â€¢	Deep Neural Network â†’ 3+ hidden layers (can go into hundreds in big architectures like ResNet, GPT, etc.)

________________________________________

Generative AI & LLMs
Generative AI uses certain aspects of Discriminative AI (pattern recognition) but goes further â€” it creates new information such as text, images, or audio.
For text data, two important model families are:
1.	LLMs (Large Language Models)
2.	Diffusion Models (e.g., Latent Diffusion Models for image generation)
Both can use special neural network architectures like GPT (Generative Pre-trained Transformer) â€” these have billions of parameters (like artificial neurons) working together.

________________________________________

Tokenization in LLMs
â€¢	LLMs donâ€™t directly understand raw text; they break text into tokens (small chunks like words or sub-words).
â€¢	Example: "Iâ€™m learning AI" â†’ [ "I", "â€™m", " learning", " AI" ]
â€¢	GPT understands a fixed vocabulary (e.g., ~50,000 tokens it has learned during training).
â€¢	If a token is out of vocabulary, the model wonâ€™t interpret it correctly.
â€¢	Context Window = the maximum number of tokens the LLM can process at once.
o	GPT-3.5 â†’ ~4k tokens
o	GPT-4 â†’ 8kâ€“32k tokens (depending on version)

________________________________________

Prompt Engineering Steps
To get the best results from an LLM, your prompt should include:
â€¢  Simulate Persona â†’ Define the role the model should take.
Example: â€œYou are an expert Python developer.â€
â€¢  Task â†’ Clearly state what you want the model to do.
Example: â€œWrite a function to reverse a string.â€
â€¢  Steps â†’ Explain how to approach the problem (break it into parts).
Example: â€œFirst, take input from the user, then reverse it, and finally print the result.â€
â€¢  Context / Constraints â†’ Give boundaries or rules for the answer.
Example: â€œDo not use built-in reverse functions; only use loops.â€
â€¢  Goal â†’ Describe the desired outcome.
Example: â€œThe program should work for any string entered by the user.â€
â€¢  Output Format â†’ Tell how you want the results presented.
Example: â€œReturn the code inside a single Python code block.â€

________________________________________

LLMs vs Diffusion Models
________________________________________

1. LLMs (Large Language Models)
LLMs are actually Transformers â€” a type of neural network architecture.

You get better results when you give clear, step-by-step prompts.

Adding emotional touch in prompts can improve the quality of generated content.

________________________________________

2. Diffusion Models
Diffusion models generate images based on text prompts.

Example: Stable Diffusion (comes under Computer Vision).

You can sell generated images, but success depends on good prompt writing.

Negative Prompts â†’ Used to exclude unwanted elements from the image.

They are computationally expensive, so latent representation is used:

Instead of representing an image with millions of numbers (pixels), itâ€™s represented with fewer (e.g., 1,000) numbers.

This makes training faster and more efficient.

________________________________________

3. Noise in Diffusion Models
   
Noise = Random variation added to the image data (like static on TV).

The model is trained to remove noise and recover the real image.

Forward process: Gradually add noise to an image over time.

Backward process: Gradually remove noise to reconstruct the image.

Example: Drawing too many lines on a picture, then removing them step-by-step until the original image appears.

________________________________________

4. Two Ways to Add/Remove Noise
  1) Equal step noise addition â†’ Add equal noise at each step, then remove it.

  2)Cosine similarity noise â†’ Uses a cosine schedule for adding noise.
    (Forward process and Reverse process)
    Cosine schedule â†’ add blur slowly at first, faster in the middle, then slow down again â€” giving a smoother transformation.
    It uses a cosine-shaped curve for adding noise

5. Latent Diffusion Components
Autoencoder â†’ Compresses image into a smaller latent space.

U-Net â†’ Learns to remove noise.

Text Autoencoder â†’ Encodes the prompt into embeddings.

Guided Noise Removal â†’ The process of removing noise in a controlled sequence.

6. Embeddings
Embeddings = Numerical representation of text prompts (meaning is stored as numbers).

The model uses embeddings to understand what the prompt means and guide image creation.











 
